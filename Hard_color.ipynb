{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b904f5-e6b4-48cb-bac2-0ccdbd6c5c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GraphColor.dataloader import ColorDataset, ColorMultiDataset, RandColoring, ColoringOneHot\n",
    "from torch_geometric.nn.models import GAT\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb\n",
    "from numpy.random import default_rng\n",
    "import math\n",
    "\n",
    "#%%\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.nn import SAGEConv, GATv2Conv, GCNConv, global_mean_pool, BatchNorm, LayerNorm\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "hypers = {\n",
    "    'num_features': 32,\n",
    "    'embedding_dim': 64,\n",
    "    'n_colors': 21,\n",
    "\n",
    "}\n",
    "def min_size(data, n):\n",
    "    return data.x.shape[0] > n\n",
    "NUM_PROCESSES = 4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "pre_transforms = torch_geometric.transforms.Compose(\n",
    "    [torch_geometric.transforms.ToUndirected()]) #, ColoringOneHot(hypers['num_features'], cat=False), RandColoring(hypers['num_features'])\n",
    "transforms = torch_geometric.transforms.Compose(\n",
    "   [ torch_geometric.transforms.ToDevice(device)])\n",
    "\n",
    "# def min_size(n, data):\n",
    "#    1\n",
    "#    return data.x.shape[0] > n\n",
    "\n",
    "\n",
    "\n",
    "filters = partial(min_size, n=50)  #curry the funtion to keep graphs with more than 50 nodes\n",
    "# torch_geometric.transforms.ComposeFilters([partial(min_size, n=50)])\n",
    "# length no filter 11929\n",
    "\n",
    "import torch\n",
    "from torch.nn import Dropout, Linear\n",
    "import torch.nn.functional as F\n",
    "# from torch.nn import Linear\n",
    "\n",
    "\n",
    "class AmazonNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Based on the Network used in Graph Coloring with Physics-Inspired Graph Neural Networks.\n",
    "    In the paper they used a 2 Conv layer Network.\n",
    "    In this approach the Conv was replaced with Transformers.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, hidden_dim, num_classes, n_heads=3):\n",
    "        super(AmazonNet, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_features, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, num_classes)\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        #x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "graph_dataset = ColorMultiDataset(root='data/', pre_transform=pre_transforms, transform=transforms, pre_filter=filters)\n",
    "for i, data in enumerate(graph_dataset):\n",
    "    try:\n",
    "        if not data.validate():\n",
    "            print(f\"Error in data entry No:{i} name:{data.name}\")\n",
    "    except ValueError:\n",
    "        print(f\"IndexError in data entry No:{i} name:{data.name}\")\n",
    "        continue\n",
    "rng = default_rng()\n",
    "choice = rng.permutation(len(graph_dataset))\n",
    "idx = math.floor(len(graph_dataset)*0.8)\n",
    "train_set = graph_dataset[0:idx]\n",
    "test_set = graph_dataset[idx:-1]\n",
    "loader_train = DataLoader(train_set, batch_size=1, shuffle=True, num_workers=0, pin_memory=False)\n",
    "loader_test = DataLoader(test_set, batch_size=1, shuffle=True, num_workers=0, pin_memory=False)\n",
    "\n",
    "print(\"...Creating Model...\")\n",
    "\n",
    "config = {\n",
    "        \"learning_rate\": 0.02,\n",
    "        'feature_rep': \"Embed:\",\n",
    "        \"dataset\": \"BA\",\n",
    "        \"epochs\": 1000,\n",
    "        \"log_interval\": 1,\n",
    "        #'NUM_ACCUMULATION_STEPS': 8,\n",
    "        \n",
    "        **hypers\n",
    "    } \n",
    "from itertools import chain\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Graphs-AAS\",\n",
    "    name=\"Recreation AmazonNet Original\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config=config\n",
    "   \n",
    ")\n",
    "\n",
    "#model = GAT(config['num_features'], config['embedding_dim'], 3 , loader_train.dataset.num_classes, jk=None)\n",
    "embed = torch.nn.Embedding(config['n_colors'], config['num_features'])\n",
    "embed.to(device)\n",
    "model = AmazonNet(config['num_features'], config['embedding_dim'], loader_train.dataset.num_classes)\n",
    "model.to(device)\n",
    "wandb.watch(model, log_freq=1)\n",
    "params = chain(model.parameters())\n",
    "optimizer = torch.optim.AdamW(params, lr=config['learning_rate'])\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c049e7-9d62-42ad-b157-c312ec0afa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"...Start Training...\")\n",
    "pre_pools = []\n",
    "post_pools = []\n",
    "first_convs = []\n",
    "names = []\n",
    "hash_tensor = torch.vmap(lambda x: x % config['n_colors'])\n",
    "def pots_loss_func(probs, adj_tensor):\n",
    "    \"\"\"\n",
    "    Function to compute cost value based on soft assignments (probabilities)\n",
    "\n",
    "    :param probs: Probability vector, of each node belonging to each class\n",
    "    :type probs: torch.tensor\n",
    "    :param adj_tensor: Adjacency matrix, containing internode weights\n",
    "    :type adj_tensor: torch.tensor\n",
    "    :return: Loss, given the current soft assignments (probabilities)\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "\n",
    "    # Multiply probability vectors, then filter via elementwise application of adjacency matrix.\n",
    "    #  Divide by 2 to adjust for symmetry about the diagonal\n",
    "    loss_ = torch.mul(adj_tensor, (probs @ probs.T)).sum() / 2\n",
    "\n",
    "    return loss_\n",
    "def loss_func_color_hard(coloring, nx_graph):\n",
    "    \"\"\"\n",
    "    Function to compute cost value based on color vector (0, 2, 1, 4, 1, ...)\n",
    "\n",
    "    :param coloring: Vector of class assignments (colors)\n",
    "    :type coloring: torch.tensor\n",
    "    :param nx_graph: Graph to evaluate classifications on\n",
    "    :type nx_graph: networkx.OrderedGraph\n",
    "    :return: Cost of provided class assignments\n",
    "    :rtype: torch.tensor\n",
    "    \"\"\"\n",
    "\n",
    "    cost_ = 0\n",
    "    for (u, v) in nx_graph.edges:\n",
    "        cost_ += 1*(coloring[u] == coloring[v])*(u != v)\n",
    "\n",
    "    return cost_\n",
    "\n",
    "\n",
    "def train(data, patience=1000, tolerance=1e-4, seed=1):\n",
    "    model.train()\n",
    "    # Tracking\n",
    "    best_loss = torch.tensor(float('Inf'))\n",
    "    best_cost = torch.tensor(float('Inf'))\n",
    "    best_coloring = None\n",
    "    prev_loss = 1.  # initial loss value (arbitrary)\n",
    "    cnt = 0  # track number times early stopping is triggered\n",
    "    adj_mat = torch_geometric.utils.to_dense_adj(data.edge_index, data.batch, max_num_nodes=data.num_nodes)\n",
    "    for epoch in range(config['epochs']):\n",
    "        #idx = torch.tensor(0, device=device)\n",
    "    \n",
    "        #input = torch.squeeze(embed(hash_tensor(data.x.long())))\n",
    "        #out, pre_pool, post_pool, _ = model(input, data.edge_index, data.batch)  # , data.batch Perform a single forward pass.\n",
    "        input = torch.squeeze(embed(hash_tensor(data.x.long())))\n",
    "        out = model(input, data.edge_index, data.batch)  # , data.batch Perform a single forward pass.\n",
    "        probas = F.softmax(out, dim=1)\n",
    "        loss = pots_loss_func(probas, adj_mat) \n",
    "        coloring = torch.argmax(probas, dim=1)\n",
    "        cost_hard = loss_func_color_hard(coloring, graph_nx)\n",
    "\n",
    "        if cost_hard < best_cost:\n",
    "            best_loss = loss\n",
    "            best_cost = cost_hard\n",
    "            best_coloring = coloring\n",
    "        if (abs(loss - prev_loss) <= tolerance) | ((loss - prev_loss) > 0):\n",
    "            cnt += 1\n",
    "        else:\n",
    "            cnt = 0\n",
    "        # update loss tracking\n",
    "        prev_loss = loss\n",
    "        if cnt >= patience:\n",
    "            print(f'Stopping early on epoch {epoch}. Patience count: {cnt}')\n",
    "            break\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        wandb.log({'train/pots_loss': loss.item(),\n",
    "                      'train/color_cost':cost_hard,\n",
    "                     })\n",
    "\n",
    "       # Final coloring\n",
    "\n",
    "    print(f'Final coloring: {coloring}, soft loss: {loss}')\n",
    "    return probas, coloring, loss, epoch\n",
    "graph_nx = torch_geometric.utils.to_networkx(graph_dataset[0])\n",
    "train(graph_dataset[0])\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07822f2-2b6b-4d0c-8269-723301f4f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dataset[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ff13d7-35f3-4f26-b0e6-6bca2e179fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "data0 = graph_dataset[0]\n",
    "data1 = graph_dataset[1]\n",
    "loss_series = []\n",
    "df_pre = []\n",
    "df_post = []\n",
    "labels = []\n",
    "first_convs = []\n",
    "for data in test_set:\n",
    "    #input = torch.squeeze(embed(hash_tensor(data.x.long())))\n",
    "    out, pre, post, first_conv = model(data.x, data.edge_index, data.batch)\n",
    "    out = F.softmax(out, dim=1)\n",
    "    loss_series.append(loss(out, data.y).cpu().detach().numpy())\n",
    "    df_pre.append(pre.cpu().detach().numpy())\n",
    "    df_post.append(post.cpu().detach().numpy())\n",
    "    labels.append(data.y.cpu().detach().numpy()[0])\n",
    "    first_convs.append(first_conv.cpu().detach().numpy())\n",
    "labels = pd.Series(labels, dtype=\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5573aec-26bb-4e93-9903-60e7efb1a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8bd9bc-8f4b-4f05-8add-13e3abacd7fb",
   "metadata": {},
   "source": [
    "This Histplot shows that our testset, ignoring the class 7,  is rather balanced.\n",
    "As such the set should be representative to all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd37c45a-1110-4e78-a511-b77c9a63be0b",
   "metadata": {},
   "source": [
    "## Investigate wether the Nodes in a Graph have different features PRE pooling\n",
    "As the graphs have different numbers of nodes they cant be stacked and have to be inspected induvidually.\n",
    "But aggregate measures can be used.\n",
    "For this the mean and the standard deviation of each node activation are taken featurewise.\n",
    "\n",
    "These graph level aggregates are then again compared agaisnt all other graphs, solverwise.\n",
    "\n",
    "This behaviour occurs with both encoding schemes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd88282-2e18-40dd-b1d0-5c15b950571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph wise std and mean of each feature dim\n",
    "first_convs_std = np.array(list(map(lambda x: np.std(x, axis=0), first_convs)))\n",
    "first_convs_mean = np.array(list(map(lambda x: np.mean(x, axis=0), first_convs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a8805-ec28-4fde-b879-58df493f30b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(first_convs_mean)\n",
    "tmp['label'] = labels\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.suptitle('Mean of activations')\n",
    "tmp.groupby(['label']).mean().T.plot(kind='bar', ax=axs[0])\n",
    "tmp.groupby(['label']).std().T.plot(kind='bar', ax=axs[1])\n",
    "for ax in axs.flat:\n",
    "    ax.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "\n",
    "plt.savefig('graphs/AmzNet_RandColoring_mean.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc12a61-1756-4039-a70e-f40c4c86594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(first_convs_std)\n",
    "tmp['label'] = labels\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.suptitle('STD of activations')\n",
    "tmp.groupby(['label']).mean().T.plot(kind='bar', ax=axs[0])\n",
    "tmp.groupby(['label']).std().T.plot(kind='bar', ax=axs[1])\n",
    "for ax in axs.flat:\n",
    "    ax.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "\n",
    "plt.savefig('graphs/AmzNet_RandColoring_std.png', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9f519b-0632-4767-9031-f9a80d1c641a",
   "metadata": {},
   "source": [
    "Across all graph labels there is no differenciation between each label.???\n",
    "From CITE we know that each layer in a GNN needs to be expressive to lead to a useful network architecture.\n",
    "Warrant is therefore needed to investiate 1 ONE good layer and not stack them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b558349-4f56-45ff-979a-84ada24fc5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "height = df_post.std()\n",
    "\n",
    "y_pos = np.arange(len(height))\n",
    "\n",
    "# Create bars\n",
    "plt.bar(y_pos, height)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e313c206-3b41-4717-81e5-631fb59d8539",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.vstack(df_post)).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e7947-8961-4d4f-8df1-10a83882434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016758dd-6be2-40d0-86ba-fd7e10d9f41e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
