{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1652352-d5e8-40ec-b57c-4a1dd72c569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b904f5-e6b4-48cb-bac2-0ccdbd6c5c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GraphColor.dataloader import ColorDataset, ColorMultiDataset, RandColoring, ColoringOneHot\n",
    "from torch_geometric.nn.models import GAT\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.transforms\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from functools import partial\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from numpy.random import default_rng\n",
    "import math\n",
    "\n",
    "#%%\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.nn import SAGEConv, GATv2Conv, GCNConv, global_mean_pool, BatchNorm, LayerNorm\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "hypers = {\n",
    "    'num_features': 32,\n",
    "    'embedding_dim': 64\n",
    "\n",
    "}\n",
    "def min_size(data, n):\n",
    "    return data.x.shape[0] > n\n",
    "NUM_PROCESSES = 4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "pre_transforms = torch_geometric.transforms.Compose(\n",
    "    [torch_geometric.transforms.ToUndirected(), RandColoring(hypers['num_features'])]) #, ColoringOneHot(hypers['num_features'], cat=False), RandColoring(hypers['num_features'])\n",
    "transforms = torch_geometric.transforms.Compose(\n",
    "   [ torch_geometric.transforms.ToDevice(device)])\n",
    "\n",
    "# def min_size(n, data):\n",
    "#    1\n",
    "#    return data.x.shape[0] > n\n",
    "\n",
    "\n",
    "\n",
    "filters = partial(min_size, n=50)  #curry the funtion to keep graphs with more than 50 nodes\n",
    "# torch_geometric.transforms.ComposeFilters([partial(min_size, n=50)])\n",
    "# length no filter 11929\n",
    "\n",
    "import torch\n",
    "from torch.nn import Dropout, Linear\n",
    "import torch.nn.functional as F\n",
    "# from torch.nn import Linear\n",
    "\n",
    "class N_GCP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Based on the Network used in Graph Coloring with Physics-Inspired Graph Neural Networks.\n",
    "    In the paper they used a 2 Conv layer Network.\n",
    "    In this approach the Conv was replaced with Transformers.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, hidden_dim, num_classes):\n",
    "        super(AmazonNet, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_features, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, num_classes)\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        #x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index) \n",
    "        x = x.relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "class AmazonNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Based on the Network used in Graph Coloring with Physics-Inspired Graph Neural Networks.\n",
    "    In the paper they used a 2 Conv layer Network.\n",
    "    In this approach the Conv was replaced with Transformers.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, hidden_dim, num_classes, n_heads=3):\n",
    "        super(AmazonNet, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_features, hidden_dim)\n",
    "        self.algo_classifier = Linear(hidden_dim, num_classes)\n",
    "        self.dropout = torch.nn.Dropout(p=0.2)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        #x, edge_index = data.x, data.edge_index\n",
    "        pre = self.conv1(x, edge_index) \n",
    "        post = global_mean_pool(pre, batch)\n",
    "        x = self.algo_classifier(post)\n",
    "        return x, pre, post\n",
    "\n",
    "\n",
    "\n",
    "graph_dataset = ColorMultiDataset(root='data/', pre_transform=pre_transforms, transform=transforms, pre_filter=filters)\n",
    "for i, data in enumerate(graph_dataset):\n",
    "    try:\n",
    "        if not data.validate():\n",
    "            print(f\"Error in data entry No:{i} name:{data.name}\")\n",
    "    except ValueError:\n",
    "        print(f\"IndexError in data entry No:{i} name:{data.name}\")\n",
    "        continue\n",
    "rng = default_rng()\n",
    "choice = rng.permutation(len(graph_dataset))\n",
    "idx = math.floor(len(graph_dataset)*0.8)\n",
    "train_set = graph_dataset[0:idx]\n",
    "test_set = graph_dataset[idx:-1]\n",
    "loader_train = DataLoader(train_set, batch_size=1, shuffle=True, num_workers=0, pin_memory=False)\n",
    "loader_test = DataLoader(test_set, batch_size=1, shuffle=True, num_workers=0, pin_memory=False)\n",
    "\n",
    "print(\"...Creating Model...\")\n",
    "\n",
    "config = {\n",
    "        \"learning_rate\": 0.02,\n",
    "        'feature_rep': \"RandColoring:\",\n",
    "        \"dataset\": \"reddit\",\n",
    "        \"epochs\": 30,\n",
    "        \"log_interval\": 1,\n",
    "        #'NUM_ACCUMULATION_STEPS': 8,\n",
    "        'n_colors': 32,\n",
    "        **hypers\n",
    "    } \n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Graphs-AAS\",\n",
    "    name=\"Recreation AmazonNet\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config=config\n",
    "   \n",
    ")\n",
    "from itertools import chain\n",
    "#model = GAT(config['num_features'], config['embedding_dim'], 3 , loader_train.dataset.num_classes, jk=None)\n",
    "embed = torch.nn.Embedding(config['n_colors'], config['num_features'])\n",
    "embed.to(device)\n",
    "#model = AmazonNet(config['num_features'], config['embedding_dim'], loader_train.dataset.num_classes, n_heads=2)\n",
    "model = N_GCP(\n",
    "model.to(device)\n",
    "wandb.watch(model, log_freq=1)\n",
    "params = chain(model.parameters(), embed.parameters())\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c049e7-9d62-42ad-b157-c312ec0afa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"...Start Training...\")\n",
    "pre_pools = []\n",
    "post_pools = []\n",
    "first_convs = []\n",
    "names = []\n",
    "hash_tensor = torch.vmap(lambda x: x % config['n_colors'])\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    \n",
    "    for data in loader:\n",
    "        #data = data.to(device)\n",
    "        \"\"\"\n",
    "        input = torch.squeeze(embed(hash_tensor(data.x.long())))\n",
    "        output = model(input, data.edge_index) #, data.batch\n",
    "        \"\"\"\n",
    "        output = model(data.x, data.edge_index, data.batch) \n",
    "        output = F.softmax(output, dim=1)\n",
    "        #color = F.softmax(color, dim=1)\n",
    "        \n",
    "        # selects the index of the max value\n",
    "        pred = output.max(dim=1)[1]\n",
    "        #truth = y = torch.flatten(torch.index_select(torch.reshape(data.y, (-1, 3)), 1, idx))\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "        \n",
    "    return {'acc': correct / len(loader.dataset)}\n",
    "def train_single(epoch):\n",
    "    model.train()\n",
    "    #idx = torch.tensor(0, device=device)\n",
    "    loss_all = 0\n",
    "    for data in loader_train:\n",
    "        #input = torch.squeeze(embed(hash_tensor(data.x.long())))\n",
    "        #out, pre_pool, post_pool, _ = model(input, data.edge_index, data.batch)  # , data.batch Perform a single forward pass.\n",
    "        out, pre_pool, post_pool  = model(data.x, data.edge_index, data.batch)  # , data.batch Perform a single forward pass.\n",
    "        out = F.softmax(out, dim=1)\n",
    "        loss = criterion(out, data.y) \n",
    "        pre_pools.append(pre_pool)\n",
    "        post_pools.append(post_pool)\n",
    "        \n",
    "        names.append(data.name)\n",
    "\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return {'train/cat_loss': loss_all / len(train_set)}\n",
    "\n",
    "for epoch in range(1, config['epochs']):\n",
    "    loss_train = train_single(epoch)\n",
    "    print(loss_train)\n",
    "    test_acc = test(loader_test)\n",
    "    wandb.log({**loss_train, **test_acc})\n",
    "    \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ff13d7-35f3-4f26-b0e6-6bca2e179fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "data0 = graph_dataset[0]\n",
    "data1 = graph_dataset[1]\n",
    "loss_series = []\n",
    "df_pre = []\n",
    "df_post = []\n",
    "labels = []\n",
    "first_convs = []\n",
    "for data in test_set:\n",
    "    #input = torch.squeeze(embed(hash_tensor(data.x.long())))\n",
    "    out, pre, post = model(data.x, data.edge_index, data.batch)\n",
    "    out = F.softmax(out, dim=1)\n",
    "    loss_series.append(loss(out, data.y).cpu().detach().numpy())\n",
    "    df_pre.append(pre.cpu().detach().numpy())\n",
    "    df_post.append(post.cpu().detach().numpy())\n",
    "    labels.append(data.y.cpu().detach().numpy()[0])\n",
    "    #first_convs.append(first_conv.cpu().detach().numpy())\n",
    "labels = pd.Series(labels, dtype=\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5573aec-26bb-4e93-9903-60e7efb1a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8bd9bc-8f4b-4f05-8add-13e3abacd7fb",
   "metadata": {},
   "source": [
    "This Histplot shows that our testset, ignoring the class 7,  is rather balanced.\n",
    "As such the set should be representative to all datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd37c45a-1110-4e78-a511-b77c9a63be0b",
   "metadata": {},
   "source": [
    "## Investigate wether the Nodes in a Graph have different features PRE pooling\n",
    "As the graphs have different numbers of nodes they cant be stacked and have to be inspected induvidually.\n",
    "But aggregate measures can be used.\n",
    "For this the mean and the standard deviation of each node activation are taken featurewise.\n",
    "\n",
    "These graph level aggregates are then again compared agaisnt all other graphs, solverwise.\n",
    "\n",
    "This behaviour occurs with both encoding schemes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd88282-2e18-40dd-b1d0-5c15b950571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph wise std and mean of each feature dim\n",
    "first_convs_std = np.array(list(map(lambda x: np.std(x, axis=0), df_pre)))\n",
    "first_convs_mean = np.array(list(map(lambda x: np.mean(x, axis=0), df_pre)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a8805-ec28-4fde-b879-58df493f30b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(first_convs_mean)\n",
    "tmp['label'] = labels\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.suptitle('Mean of activations')\n",
    "tmp.groupby(['label']).mean().T.plot(kind='bar', ax=axs[0])\n",
    "tmp.groupby(['label']).std().T.plot(kind='bar', ax=axs[1])\n",
    "for ax in axs.flat:\n",
    "    ax.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "\n",
    "plt.savefig('invest_RandColoring_mean.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc12a61-1756-4039-a70e-f40c4c86594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(first_convs_std)\n",
    "tmp['label'] = labels\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.suptitle('STD of activations')\n",
    "tmp.groupby(['label']).mean().T.plot(kind='bar', ax=axs[0])\n",
    "tmp.groupby(['label']).std().T.plot(kind='bar', ax=axs[1])\n",
    "for ax in axs.flat:\n",
    "    ax.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "\n",
    "plt.savefig('invest_RandColoring_std.png', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9f519b-0632-4767-9031-f9a80d1c641a",
   "metadata": {},
   "source": [
    "Across all graph labels there is no differenciation between each label.???\n",
    "From CITE we know that each layer in a GNN needs to be expressive to lead to a useful network architecture.\n",
    "Warrant is therefore needed to investiate 1 ONE good layer and not stack them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016758dd-6be2-40d0-86ba-fd7e10d9f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd7d307-467a-4af5-ad09-1fed5ace0e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
